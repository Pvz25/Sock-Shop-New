# INCIDENT-8A: Database Performance Degradation (CORRECT IMPLEMENTATION)

## Overview

**Incident Type:** Database Performance Degradation - Query Latency  
**Severity:** High (P2)  
**User Impact:** Product search slowness (2-10 seconds per page)  
**Root Cause:** Database query latency due to table lock simulation  
**Client Requirement:** ‚úÖ "Product search slowness due to database latency or connection pool exhaustion"

---

## Why INCIDENT-8A Exists

### INCIDENT-8 (Original) - Why It Failed

**Method:** Resource constraints (CPU/memory limits)  
**Result:** Database crashes (OOMKilled), not slowness  
**Problem:** Cannot create slowness by starving resources

### INCIDENT-8A (Corrected) - How It Works

**Method:** Artificial query delay using MySQL table locks  
**Result:** Queries complete, but take 5-10 seconds  
**Success:** Real slowness without crashes ‚úÖ

---

## Technical Approach

### The Challenge

**Goal:** Make database queries slow WITHOUT crashing the database

**Cannot Use:**
- ‚ùå Resource constraints ‚Üí Causes OOMKill
- ‚ùå Killing database ‚Üí Causes complete failure
- ‚ùå Network delays ‚Üí Affects all services

**Must Use:**
- ‚úÖ Database-level mechanisms that delay queries
- ‚úÖ Queries eventually complete (slow, not failed)
- ‚úÖ Database remains healthy

### The Solution: Table-Level Read Lock

**MySQL Mechanism:** `LOCK TABLES ... READ`

**How It Works:**
```sql
-- Session 1: Acquire read lock (simulates long-running query/maintenance)
LOCK TABLES sock READ;
SELECT SLEEP(300);  -- Hold lock for 5 minutes

-- Session 2-N: All SELECT queries wait for lock
SELECT * FROM sock;  -- Waits... waits... waits... (5-10 seconds)
```

**Result:**
- ‚úÖ Database is healthy (not crashed)
- ‚úÖ Queries DO complete (eventually)
- ‚è≥ But take 5-10 seconds (frustrating delay)
- ‚úÖ Simulates: Maintenance operation, long-running query, index rebuild

---

## Architecture Flow

### Normal Operation (Baseline)

```mermaid
sequenceDiagram
    participant User
    participant FE as Front-end
    participant Cat as Catalogue Service
    participant DB as Catalogue-DB (MariaDB)

    User->>FE: Browse Products
    FE->>Cat: GET /catalogue
    Cat->>DB: SELECT * FROM sock
    Note over DB: Query executes in 50ms ‚ö°
    DB-->>Cat: Result set (6 products)
    Cat-->>FE: JSON response (200 OK)
    FE-->>User: Products displayed ‚úÖ
    Note over User: Page loads in <1 second
```

### Degraded Performance (INCIDENT-8A Active)

```mermaid
sequenceDiagram
    participant User
    participant FE as Front-end
    participant Cat as Catalogue Service
    participant DB as Catalogue-DB (MariaDB)
    participant Lock as Lock Holder Session

    Note over Lock,DB: Table lock held by maintenance session
    Lock->>DB: LOCK TABLES sock READ<br/>SELECT SLEEP(300)
    
    User->>FE: Browse Products
    FE->>Cat: GET /catalogue
    Cat->>DB: SELECT * FROM sock
    Note over DB: Query WAITS for lock üêå<br/>(5-10 seconds)
    Note over Cat: Waiting for database...
    Note over FE: Waiting for catalogue...
    Note over User: Loading... Loading... üò§
    
    DB-->>Cat: Result set (after 8 seconds)
    Cat-->>FE: JSON response (200 OK)
    FE-->>User: Products displayed ‚úÖ
    Note over User: Page loads in 8-10 seconds<br/>Frustrating!
```

---

## Pre-Incident Checklist

### 1. Verify Baseline Performance

```powershell
# Ensure front-end accessible
kubectl port-forward -n sock-shop svc/front-end 2025:80

# Test in browser: http://localhost:2025
# Products should load in <1 second ‚ö°
```

### 2. Verify Database Health

```powershell
# Check catalogue-db pod
kubectl get pods -n sock-shop -l name=catalogue-db

# Expected: Running, no restarts
```

### 3. Test Database Connection

```powershell
# Connect to database
kubectl exec -it -n sock-shop deployment/catalogue-db -- mysql -u root -padmin socksdb

# Run test query
SELECT COUNT(*) FROM sock;
# Should return 6 instantly
```

---

## Triggering INCIDENT-8A

### Step 1: Open Database Session (Lock Holder)

```powershell
# Open persistent session to catalogue-db
kubectl exec -it -n sock-shop deployment/catalogue-db -- mysql -u root -padmin socksdb
```

**Keep this terminal open!** This session will hold the lock.

### Step 2: Acquire Table Lock

**In the MySQL session:**

```sql
-- Document incident start
SELECT NOW() AS incident_start;

-- Acquire read lock on sock table
LOCK TABLES sock READ;

-- Hold lock for 5 minutes (simulates maintenance operation)
SELECT SLEEP(300);
```

**What this does:**
- Locks the `sock` table for reading
- All other SELECT queries must WAIT for this lock
- Simulates: Database maintenance, long-running query, index rebuild

**CRITICAL:** Do NOT close this terminal! The lock is held by this session.

### Step 3: Verify Lock is Active

**Open a second terminal:**

```powershell
# Check for table locks
kubectl exec -it -n sock-shop deployment/catalogue-db -- mysql -u root -padmin -e "SHOW OPEN TABLES WHERE In_use > 0;"
```

**Expected output:**
```
+----------+------+--------+-------------+
| Database | Table| In_use | Name_locked |
+----------+------+--------+-------------+
| socksdb  | sock |      1 |           0 |
+----------+------+--------+-------------+
```

---

## User Experience Testing

### Step 4: Test in UI (Your 45-Second Window!)

**Open browser:** http://localhost:2025

**Test these actions:**

1. **Click "Catalogue"**
   - Page will LOAD (not blank)
   - But takes 5-10 seconds üêå
   - Products eventually appear ‚úÖ

2. **Refresh Page (F5)**
   - Each refresh: 5-10 second delay
   - Consistent slowness
   - Products always load (just slow)

3. **Click on Product**
   - Product details load slowly
   - 5-10 second delay

4. **Search Products**
   - Search works but is very slow
   - Results appear after 5-10 seconds

**What You're Experiencing:**
- ‚è≥ Noticeable, frustrating delays
- ‚úÖ Products DO load (not blank)
- üêå Every database query takes 5-10 seconds
- ‚úÖ This is REAL slowness, not crash

---

## Datadog Signals

### What AI SRE Should Detect

**1. Query Latency Spike:**
```
mysql.performance.query_run_time.avg
‚Üí Increases from 50ms to 5000-10000ms
```

**2. Queries Waiting for Locks:**
```
mysql.performance.table_lock_waits
‚Üí Increases significantly
```

**3. Service Response Time:**
```
http.request.duration{service:catalogue}
‚Üí P95: 8000-12000ms (was 100ms)
‚Üí P99: 10000-15000ms
```

**4. No Error Rate Increase:**
```
http.errors{service:catalogue}
‚Üí Remains LOW (queries succeed, just slow)
```

**5. Database Connections:**
```
mysql.performance.threads_connected
‚Üí May increase (connections waiting)
```

---

## Recovery

### Step 5: Release Table Lock

**In the original MySQL session (where you ran LOCK TABLES):**

```sql
-- Stop the SLEEP (Ctrl+C if still running)
-- Then unlock tables
UNLOCK TABLES;

-- Verify lock released
SHOW OPEN TABLES WHERE In_use > 0;
-- Should show empty result

-- Exit
EXIT;
```

### Step 6: Verify Recovery

**Test in browser:**
- Refresh http://localhost:2025
- Products should load INSTANTLY (<1 second) ‚úÖ
- No more delays

**Check query performance:**
```powershell
kubectl exec -it -n sock-shop deployment/catalogue-db -- mysql -u root -padmin socksdb -e "SELECT * FROM sock LIMIT 1;"
# Should return instantly
```

---

## Why This Works

### Comparison Table

| Aspect | INCIDENT-8 (Failed) | INCIDENT-8A (Success) |
|--------|---------------------|----------------------|
| **Method** | Resource constraints | Table locks |
| **Database Status** | Crashed (OOMKilled) | Healthy ‚úÖ |
| **Query Result** | Failed/Timeout ‚ùå | Success (slow) ‚úÖ |
| **User Experience** | Blank pages | Slow loading ‚è≥ |
| **Products Display** | No ‚ùå | Yes (delayed) ‚úÖ |
| **Matches Client Req** | No ‚ùå | Yes ‚úÖ |

### Real-World Scenarios This Simulates

1. **Database Maintenance:**
   - Index rebuild running during business hours
   - Table optimization in progress
   - Backup operation holding locks

2. **Long-Running Query:**
   - Analytics query scanning entire table
   - Report generation holding read lock
   - Data export operation

3. **Lock Contention:**
   - Multiple services competing for same table
   - Write-heavy workload blocking reads
   - Deadlock resolution delays

---

## AI SRE Learning Objectives

### What AI Agent Should Learn

**1. Detect Database Latency:**
- Query execution time spike (50ms ‚Üí 8000ms)
- Table lock wait time increase
- Connection pool saturation

**2. Correlate Symptoms:**
- Database slow ‚Üí Service slow ‚Üí User impact
- No errors, just latency
- All queries to same table affected

**3. Root Cause Analysis:**
- Identify: Table locks held
- Distinguish: Latency vs crash vs failure
- Recognize: Maintenance operation pattern

**4. Remediation Strategy:**
- Immediate: Kill long-running query
- Short-term: Schedule maintenance off-peak
- Long-term: Optimize queries, add read replicas

**5. Business Impact:**
- Slow product browsing = revenue loss
- User abandonment after 3 seconds
- Severity: HIGH (affects all users)

---

## Automation Scripts

### incident-8a-activate.ps1

```powershell
# INCIDENT-8A: Activate Database Slowness
Write-Host "Activating INCIDENT-8A: Database Query Latency" -ForegroundColor Cyan

# Start background job to hold table lock
$lockJob = Start-Job -ScriptBlock {
    kubectl exec -i -n sock-shop deployment/catalogue-db -- mysql -u root -padmin socksdb <<EOF
SELECT NOW() AS incident_start;
LOCK TABLES sock READ;
SELECT SLEEP(300);
UNLOCK TABLES;
EOF
}

Write-Host "‚úÖ Table lock acquired on 'sock' table" -ForegroundColor Green
Write-Host "‚è≥ All queries will now take 5-10 seconds" -ForegroundColor Yellow
Write-Host ""
Write-Host "Test in UI: http://localhost:2025" -ForegroundColor Cyan
Write-Host "Products will load slowly (5-10 seconds)" -ForegroundColor Yellow
Write-Host ""
Write-Host "To recover: .\incident-8a-recover.ps1" -ForegroundColor White
```

### incident-8a-recover.ps1

```powershell
# INCIDENT-8A: Recover from Database Slowness
Write-Host "Recovering from INCIDENT-8A..." -ForegroundColor Cyan

# Kill the lock-holding session
kubectl exec -i -n sock-shop deployment/catalogue-db -- mysql -u root -padmin -e "SHOW PROCESSLIST;" | Select-String "SLEEP" | ForEach-Object {
    $processId = ($_ -split '\s+')[1]
    kubectl exec -i -n sock-shop deployment/catalogue-db -- mysql -u root -padmin -e "KILL $processId;"
}

Write-Host "‚úÖ Table locks released" -ForegroundColor Green
Write-Host "‚úÖ Database performance restored" -ForegroundColor Green
Write-Host ""
Write-Host "Test in UI: http://localhost:2025" -ForegroundColor Cyan
Write-Host "Products should load instantly now" -ForegroundColor Green
```

---

## Troubleshooting

### Issue: Lock Not Working

**Check if lock is held:**
```sql
SHOW OPEN TABLES WHERE In_use > 0;
```

**If empty, re-acquire lock:**
```sql
LOCK TABLES sock READ;
SELECT SLEEP(300);
```

### Issue: Still No Slowness

**Verify catalogue service is querying database:**
```powershell
kubectl logs -n sock-shop -l name=catalogue --tail=20
# Should see database queries
```

**Test direct database query:**
```powershell
# Should be slow (5-10 seconds)
kubectl exec -it -n sock-shop deployment/catalogue-db -- mysql -u root -padmin socksdb -e "SELECT * FROM sock;"
```

### Issue: Database Crashed

**This should NOT happen with table locks.**

**If it does:**
```powershell
# Check pod status
kubectl get pods -n sock-shop -l name=catalogue-db

# If crashed, restart
kubectl rollout restart deployment/catalogue-db -n sock-shop
```

---

## Success Criteria

### ‚úÖ Incident is Working If:

1. **Database pod is Running** (not crashed)
2. **Products DO load in UI** (not blank)
3. **But loading takes 5-10 seconds** (noticeable delay)
4. **Queries eventually complete** (not timeout)
5. **Consistent slowness** (every page load is slow)

### ‚ùå Incident Failed If:

1. Database pod crashed/restarting
2. Blank pages in UI
3. Error messages
4. Products load instantly (lock not working)

---

## Client Requirement Satisfaction

**Client Asked For:**
> "Product search slowness due to database latency or connection pool exhaustion"

**INCIDENT-8A Delivers:**
- ‚úÖ Product search IS slow (5-10 seconds)
- ‚úÖ Due to database latency (table lock wait)
- ‚úÖ Simulates connection pool exhaustion (queries waiting)
- ‚úÖ Products DO load (just slowly)
- ‚úÖ Real-world scenario (maintenance operation)
- ‚úÖ AI SRE can detect and diagnose

**Verdict:** ‚úÖ **FULLY SATISFIES CLIENT REQUIREMENT**

---

## Comparison: All Approaches

| Method | Slowness | Crash | Realistic | Easy | Verdict |
|--------|----------|-------|-----------|------|---------|
| Resource constraints (INCIDENT-8) | ‚ùå | ‚úÖ | ‚ùå | ‚úÖ | Failed |
| Table locks (INCIDENT-8A) | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | **Success** |
| Load testing (Locust) | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå | Good (requires tool) |
| Service throttling (INCIDENT-4) | ‚úÖ | ‚ùå | ‚ö†Ô∏è | ‚úÖ | OK (wrong layer) |

---

## Execution Summary

**To activate INCIDENT-8A:**

1. Open MySQL session: `kubectl exec -it -n sock-shop deployment/catalogue-db -- mysql -u root -padmin socksdb`
2. Run: `LOCK TABLES sock READ; SELECT SLEEP(300);`
3. Test UI: http://localhost:2025 (products load slowly)
4. Recover: `UNLOCK TABLES;` or run `incident-8a-recover.ps1`

**Duration:** 5 minutes (or until you unlock)  
**Impact:** Slowness, not crash  
**Reversible:** Instant (unlock tables)  
**Risk:** Zero (just a read lock)

---

**This is the CORRECT solution for database slowness simulation.** üéØ
